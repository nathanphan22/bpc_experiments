
# Introduction

This repository regroups the scipts used during the bin-picking challenge from OpenCV.
Along with the scripts are some files coming directly form the training dataset of the challenge of outputs from the scripts.

Following is an brief explanation of the purpose of each file.

# Models Visualization

This program enables intuitive visualization of 3D object poses from an annotated dataset. It allows a user to select a scene image and click on visible objects within it to display their original and rotated 3D mesh representations side-by-side.

## Pipeline Breakdown

1. **User Image Selection**:
   - A GUI prompts the user to select an image.
   - Scene, camera, and photo ID are inferred from the image path.
  
2. **Image Display**:
   - The selected image is shown using `matplotlib`.
   - Clicking on an object triggers a callback.
   - On click, the program searches the nearest object's 2D bounding box center (`bbox_obj`) using Euclidean distance.

3. **Pose Extraction**:
   - From `scene_gt_camX.json`, the object’s rotation (`R_mat`) and translation (`T_mat`) are retrieved.
   - The object ID is also obtained.

4. **3D Visualization**:
   - The object's neutral mesh is loaded from `.ply`.
   - A rotated copy is created using `R_mat`.
   - Both meshes are offset for visual separation and colored differently.
   - A coordinate frame is added for spatial reference.


## How to Run

1. **Install dependencies**:
   ```bash
   pip install open3d matplotlib numpy
   ```

2. **Launch the script**:
   ```bash
   Models Visualization.py
   ```

3. **Select an image** (from the `ipd/train_pbr` structure).
4. **Click on any object in the image to visualize its 3D pose**.

## Requirements

- Python 3.6+
- `Open3D`
- `matplotlib`
- `numpy`

# Custom Image Generation for 3D Models

This program generates synthetic images of 3D objects by rendering them in random orientations. The images are created with a uniform gray background and saved in a specified directory. Rotation matrices for each generated scene are also stored in a separate text file for further use or analysis.

## Pipeline Breakdown

### Input Files and Directories

- **3D Model Files**: The program searches 3D models directly in the `ipd/models` directory.
- **Output Images**: The rendered images are saved in subdirectories within a base directory (`custom_images`).
- **Rotation Matrices**: A `.txt` file for each model is created, storing the corresponding rotation matrices used for each generated image.

### Rendering Process

1. **Model Loading**: The script loads each model in `.ply` format from the `ipd/models` directory.
2. **Random Rotations**: `n` random rotation matrices are generated for each model.
3. **Camera Setup**: The camera is positioned based on the bounding box of the model, ensuring the object is always centered and fully visible.
4. **Rendering**: The object is rendered with each rotation, and the resulting images are saved.
5. **Rotation Matrix Saving**: For each generated image, the rotation matrix is saved in a text file.

### Directory Structure

```
custom_images/
├── 000000/
│   ├── rot_000.png
│   ├── rot_001.png
│   └── rot_002.png
│   ...
├── 000001/
│   ├── rot_000.png
│   ├── rot_001.png
│   └── rot_002.png
│   ...
...
├── rotation_matrices/
│   ├── 000000.txt
│   └── 000001.txt
│   ...
```

## How to Run

1. **Install Dependencies**:
   ```bash
   pip install open3d scipy pillow numpy
   ```

2. **Launch the Script**:
   ```bash
   python Cstom Image Generation.py
   ```

3. **Parameters**:
   - `n`: Number of random rotations (default is 2000).

## Requirements

- Python 3.6+
- `Open3D`
- `scipy`
- `numpy`
- `Pillow`

# Custom Image Contour Making

This program analyzes images of 3D objects generated from the previous script, extracting the external contours of the objects' shapes in the 2D plane. The contours are saved as polygons, and both the original and contour images are stored in separate files for each model.

## Pipeline Breakdown

### Input Files and Directories

- **Input Images**: The program expects images generated by the previous script in the `custom_images` directory.
- **Output Contours**: The extracted contours are saved in separate text files within the `custom_images_contours` directory.
- **Polygons**: For each model, the coordinates of the contour points are saved in a `.txt` file.

### Processing Steps

1. **Image Loading**: The program loads each rendered image from the `custom_images` directory.
2. **Edge Detection**: It uses the Canny edge detection algorithm to detect the external contours of the object.
3. **Contour Extraction**: The external contours are extracted using OpenCV's `findContours` function.
4. **Contour Saving**: The contours are approximated to simplify their shape, and the 2D coordinates are saved in a text file.
5. **Contour Images**: The program saves an image where only the contours of the object are visible, drawn on a blank canvas.

### Directory Structure

```
custom_images_contours/
├── 000000/
│   ├── rot_000.png
│   ├── rot_001.png
│   └── rot_002.png
│   ...
├── 000001/
│   ├── rot_000.png
│   ├── rot_001.png
│   └── rot_002.png
│   ...
...
├── polygons/
│   ├── 000000/
│   │   ├── rot_000.txt
│   │   └── rot_001.txt
│   │   ...
│   ├── 000001/
│   │   ├── rot_000.txt
│   │   └── rot_001.txt
│   │   ...
│   ...
```

## How to Run

1. **Install Dependencies**:
   ```bash
   pip install open3d scipy pillow numpy opencv-python
   ```

2. **Launch the Script**:
   ```bash
   python Custom Images Contour Making.py
   ```

## Requirements

- Python 3.6+
- `Open3D`
- `scipy`
- `numpy`
- `Pillow`
- `OpenCV`

# Preliminary Dataset: Polygon Annotation Formatter for SAM

This script converts raw polygon annotations into a structured format compatible with Meta AI's **Segment Anything Model (SAM)**. It processes both contour data stored in `.txt` files and the corresponding images to produce JSON annotation files and standardized image names.

## Pipeline Breakdown

1. **Folder Setup**:
   - Creates two output directories:
     - `annotations/` for JSON files.
     - `images/` for processed image files.

2. **Polygon Annotation Parsing**:
   - Iterates through `custom_images_contours/polygons/`:
     - Each subfolder represents a class of objects.
     - Each `.txt` file contains polygon points for one object.
   - For each file:
     - Reads and parses x, y coordinates.
     - Builds a dictionary with metadata and a list of polygon points.
     - Outputs a JSON file named `[class]-[image].json`.

3. **Image Processing**:
   - Iterates through `custom_images/`:
     - Each subfolder contains the actual image files for each object class.
   - For each `.png`:
     - Loads and resaves the image to `images/` with a consistent naming convention: `[class]-[filename].png`.

4. **Annotation Format**:
   - Each output JSON includes:
     - `image_path`: relative path to the image file.
     - `height` and `width`: hardcoded to `480x640`.
     - `masks`: list with one polygon entry:
       - `label`: includes the object class.
       - `points`: list of `[x, y]` coordinate pairs.

## How to Run

1. **Install required packages**:
   ```bash
   pip install pillow

2. **Make sure the custom image generation script has generated its output.**

3. **Launch the script**:
   ```bash
   Preliminary Dataset.py
   ```
   
# Contours for Training Images

This program automates the generation of contour-based polygon annotations for training images by rendering each image with a single 3D object model using its camera and object pose parameters. The generated annotations are saved in JSON format, suitable for training segmentation models such as SAM.

## Pipeline Breakdown

1. **Image Iteration**:
   - Iterates over each image in the training dataset located in `ipd/train_pbr/`.
   - For each scene, camera, and image index:
     - Loads camera intrinsics and extrinsics.
     - Loads the pose and ID of each object in the scene.

2. **Scene Rendering (Per Object)**:
   - Renders the scene offscreen using Open3D's `OffscreenRenderer` with:
     - Only one object mesh loaded at a time.
     - Camera set up from `scene_camera` JSON file.
   - Renders at reduced resolution for efficiency, then rescales contours later.

3. **Contour Extraction**:
   - Converts rendered image to grayscale.
   - Applies Gaussian blur and Canny edge detection to isolate object edges.
   - Extracts and simplifies external contours using `cv2.approxPolyDP`.

4. **Contour Post-processing**:
   - Applies convex hull to combine overlapping segments from multiple contours.
   - Ensures a unified polygon contour per object.

5. **Annotation Generation**:
   - Stores each object’s contour as a polygon under the key `masks` in a JSON object.
   - Includes `image_path`, `height`, and `width` metadata.

6. **Image Saving**:
   - Copies the original `.jpg` image into `training_dataset_images/` with a standardized name.

---

## Example JSON Output

```json
{
  "image_path": "training_dataset_images/000000_cam1_000001.jpg",
  "height": 2400,
  "width": 2400,
  "masks": [
    {
      "label": "obj_000012",
      "points": [[x1, y1], [x2, y2], ...]
    },
    ...
  ]
}
```

## How to Run

1. **Install dependencies**:
   ```bash
   pip install open3d numpy opencv-python scipy pillow
   ```

2. **Launch the script**:
   ```bash
   Contours for Training Images.py
   ```

NB : In our case, the programm stopped filling the annotation files after two scenes, probably due to an overflow in the system memory. 
To get around this issue, we launched the script with the instruction of doing one scene at a time.

## Requirements

- Python 3.6+
- `Open3D`
- `numpy`
- `OpenCV`
- `SciPy`
- `Pillow`

